{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a30683f-062e-4576-85b6-d6b99867b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe18301-2674-43c9-b631-704d67af04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ml25m_data(output_path: comp.OutputPath(str)):\n",
    "    import requests\n",
    "    from tqdm import tqdm\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-25m.zip'\n",
    "    response = requests.get(url, stream=True, verify=False)\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "    progress_bar = tqdm(total=file_size, unit=\"B\", unit_scale=True)\n",
    "    print(output_path)\n",
    "    with open(output_path, 'wb') as file: \n",
    "        for chunk in response.iter_content(chunk_size=1024*2): #D\n",
    "            # Update the progress bar with the size of the downloaded chunk #D\n",
    "            progress_bar.update(len(chunk)) #D\n",
    "            file.write(chunk)\n",
    "            \n",
    "def unzip_data(input_path: comp.InputPath(str), ratings_output_path: comp.OutputPath(str), movies_output_path: comp.OutputPath(str)):\n",
    "    import zipfile\n",
    "\n",
    "    with zipfile.ZipFile(input_path, 'r') as z:\n",
    "        with open(ratings_output_path, 'wb') as f:\n",
    "            f.write(z.read('ml-25m/ratings.csv'))\n",
    "        with open(movies_output_path, 'wb') as f:\n",
    "            f.write(z.read('ml-25m/movies.csv'))\n",
    "    \n",
    "def split_dataset(input_parquet: comp.InputPath(str), dataset_path: comp.OutputPath(str), random_state: int = 42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "    ratings_df = pd.read_parquet(input_parquet)\n",
    "\n",
    "    # train is now 75% of the entire data set\n",
    "    train, test = train_test_split(\n",
    "        ratings_df,                                    \n",
    "        test_size=1 - train_ratio,\n",
    "        random_state=random_state)\n",
    "\n",
    "    # test is now 10% of the initial data set\n",
    "    # validation is now 15% of the initial data set\n",
    "    val, test = train_test_split(   \n",
    "        test,\n",
    "        test_size=test_ratio / (test_ratio + validation_ratio),\n",
    "        random_state=random_state)\n",
    "    os.mkdir(dataset_path)\n",
    "    train.to_parquet(os.path.join(dataset_path, 'train.parquet.gzip'), compression='gzip')\n",
    "    test.to_parquet(os.path.join(dataset_path, 'test.parquet.gzip'), compression='gzip')\n",
    "    val.to_parquet(os.path.join(dataset_path, 'val.parquet.gzip'), compression='gzip')\n",
    "\n",
    "def csv_to_parquet(inputFile: comp.InputPath(str), output_path: comp.OutputPath(str)):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(inputFile, index_col=False)\n",
    "    df.to_parquet(output_path, compression='gzip') \n",
    "    \n",
    "def put_to_minio(inputFile: comp.InputPath(str), upload_file_name:str='', bucket: str='datasets'):\n",
    "    import boto3\n",
    "    import os\n",
    "    minio_client = boto3.client(                          \n",
    "        's3',                                              \n",
    "        endpoint_url='http://minio-service.kubeflow:9000',\n",
    "        aws_access_key_id='minio',\n",
    "        aws_secret_access_key='minio123') \n",
    "    try:\n",
    "        minio_client.create_bucket(Bucket=bucket)\n",
    "    except Exception as e:\n",
    "        # Bucket already created.\n",
    "        pass\n",
    "    if os.path.isdir(inputFile):\n",
    "        for file in os.listdir(inputFile):\n",
    "            s3_path = os.path.join('ml-25m', file)\n",
    "            minio_client.upload_file(os.path.join(inputFile, file), bucket, s3_path)\n",
    "    else:\n",
    "        if upload_file_name == '':\n",
    "            _, file = os.path.split(inputFile)\n",
    "        else:\n",
    "            file = upload_file_name\n",
    "        s3_path = os.path.join('ml-25m', file)\n",
    "        minio_client.upload_file(inputFile, bucket, s3_path)\n",
    "        \n",
    "def qa_data(bucket:str = 'datasets', dataset:str = 'ml-25m'):\n",
    "    from pyarrow import fs, parquet\n",
    "    print(\"Running QA\")\n",
    "    minio = fs.S3FileSystem(\n",
    "        endpoint_override='http://minio-service.kubeflow:9000',\n",
    "         access_key='minio',\n",
    "         secret_key='minio123',\n",
    "         scheme='http')\n",
    "    train_parquet = minio.open_input_file(f'{bucket}/{dataset}/train.parquet.gzip')\n",
    "    df = parquet.read_table(train_parquet).to_pandas()\n",
    "    assert df.shape[1] == 4\n",
    "    assert df.shape[0] >= 0.75 * 25 * 1e6\n",
    "    print('QA passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d8b838-72c8-4ca5-85bf-7492edbe35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = comp.create_component_from_func(download_ml25m_data, output_component_file='download_ml25m_component.yaml', packages_to_install=[\"requests\", \"tqdm\"])\n",
    "unzip_op = comp.create_component_from_func(unzip_data, output_component_file='unizip_data.yaml')\n",
    "csv_to_parquet_op = comp.create_component_from_func(csv_to_parquet, output_component_file='csv_to_paraquet.yaml', packages_to_install=[\"pandas\", \"fastparquet\"])\n",
    "split_dataset_op = comp.create_component_from_func(split_dataset, output_component_file='split_dataset.yaml', packages_to_install=[\"scikit-learn\", \"pandas\", \"fastparquet\"])\n",
    "upload_to_minio_op = comp.create_component_from_func(put_to_minio, output_component_file='put_to_minio.yaml', packages_to_install=[\"boto3\"])\n",
    "qa_component_op = comp.create_component_from_func(qa_data, output_component_file='qa_component.yaml', packages_to_install=[\"pyarrow\", \"pandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ae8c37-cd24-483f-8c02-ad806f0d32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "client = kfp.Client() # change arguments accordingly\n",
    "@dsl.pipeline(\n",
    "  name='Data prep pipeline',\n",
    "  description='A pipeline that retrieves data from movielens and ingests it into paraquet files on minio'\n",
    ")\n",
    "def dataprep_pipeline(minio_bucket:str='datasets', random_init:int=42):\n",
    "    download_dataset = download_op()\n",
    "    unzip_folder = unzip_op(download_dataset.output)\n",
    "    ratings_parquet_op = csv_to_parquet_op(unzip_folder.outputs['ratings_output'])\n",
    "    movies_parquet_op = csv_to_parquet_op(unzip_folder.outputs['movies_output'])\n",
    "    split_op = split_dataset_op(ratings_parquet_op.output,random_state=random_init)\n",
    "    u1 = upload_to_minio_op(movies_parquet_op.output, upload_file_name='movies.parquet.gzip', bucket=minio_bucket)\n",
    "    u2 = upload_to_minio_op(split_op.output, bucket=minio_bucket)\n",
    "    qa_component_op(bucket=minio_bucket).after(u2)\n",
    "\n",
    "# Create a pipeline run, using the client you initialized in a prior step.\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=dataprep_pipeline,\n",
    "    package_path='dataPrep_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f89e6-2476-41b7-afa1-aac9235108ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = client.get_pipeline_id(name='ml-25m-processing')\n",
    "if s:\n",
    "    client.delete_pipeline(pipeline_id=s)\n",
    "pipeline = client.pipeline_uploads.upload_pipeline('dataPrep_pipeline.yaml', name='ml-25m-processing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
